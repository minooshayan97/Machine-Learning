{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80903</td>\n",
       "      <td>0.56355</td>\n",
       "      <td>0.28385</td>\n",
       "      <td>417</td>\n",
       "      <td>416</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0706</td>\n",
       "      <td>3.0190</td>\n",
       "      <td>3.1212</td>\n",
       "      <td>2.4921</td>\n",
       "      <td>3.5844</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>3.3805</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>6.8671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16084</td>\n",
       "      <td>0.56499</td>\n",
       "      <td>0.59194</td>\n",
       "      <td>415</td>\n",
       "      <td>413</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9704</td>\n",
       "      <td>1.7451</td>\n",
       "      <td>1.8277</td>\n",
       "      <td>2.4976</td>\n",
       "      <td>5.2981</td>\n",
       "      <td>4.2616</td>\n",
       "      <td>6.3042</td>\n",
       "      <td>10.9058</td>\n",
       "      <td>28.4170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88389</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.46815</td>\n",
       "      <td>381</td>\n",
       "      <td>380</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>...</td>\n",
       "      <td>51.5607</td>\n",
       "      <td>44.4641</td>\n",
       "      <td>26.1586</td>\n",
       "      <td>6.3076</td>\n",
       "      <td>2.8601</td>\n",
       "      <td>2.5361</td>\n",
       "      <td>3.5377</td>\n",
       "      <td>3.3545</td>\n",
       "      <td>5.0424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83782</td>\n",
       "      <td>0.74890</td>\n",
       "      <td>0.49823</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>...</td>\n",
       "      <td>19.1607</td>\n",
       "      <td>12.8312</td>\n",
       "      <td>8.9434</td>\n",
       "      <td>2.2044</td>\n",
       "      <td>1.9496</td>\n",
       "      <td>1.9664</td>\n",
       "      <td>2.6801</td>\n",
       "      <td>2.8332</td>\n",
       "      <td>3.7131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.81304</td>\n",
       "      <td>0.76471</td>\n",
       "      <td>0.46374</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>...</td>\n",
       "      <td>62.9927</td>\n",
       "      <td>21.8152</td>\n",
       "      <td>9.2457</td>\n",
       "      <td>4.8555</td>\n",
       "      <td>3.0551</td>\n",
       "      <td>3.0415</td>\n",
       "      <td>4.0116</td>\n",
       "      <td>2.6217</td>\n",
       "      <td>3.1527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 755 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0      0       1  0.85247  0.71826  0.57227        240               239   \n",
       "1      0       1  0.76686  0.69481  0.53966        234               233   \n",
       "2      0       1  0.85083  0.67604  0.58982        232               231   \n",
       "3      1       0  0.41121  0.79672  0.59257        178               177   \n",
       "4      1       0  0.32790  0.79782  0.53028        236               235   \n",
       "..   ...     ...      ...      ...      ...        ...               ...   \n",
       "751  250       0  0.80903  0.56355  0.28385        417               416   \n",
       "752  250       0  0.16084  0.56499  0.59194        415               413   \n",
       "753  251       0  0.88389  0.72335  0.46815        381               380   \n",
       "754  251       0  0.83782  0.74890  0.49823        340               339   \n",
       "755  251       0  0.81304  0.76471  0.46374        340               339   \n",
       "\n",
       "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  ...  \\\n",
       "0            0.008064            0.000087       0.00218  ...   \n",
       "1            0.008258            0.000073       0.00195  ...   \n",
       "2            0.008340            0.000060       0.00176  ...   \n",
       "3            0.010858            0.000183       0.00419  ...   \n",
       "4            0.008162            0.002669       0.00535  ...   \n",
       "..                ...                 ...           ...  ...   \n",
       "751          0.004627            0.000052       0.00064  ...   \n",
       "752          0.004550            0.000220       0.00143  ...   \n",
       "753          0.005069            0.000103       0.00076  ...   \n",
       "754          0.005679            0.000055       0.00092  ...   \n",
       "755          0.005676            0.000037       0.00078  ...   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                       1.5620                     2.6445   \n",
       "1                       1.5589                     3.6107   \n",
       "2                       1.5643                     2.3308   \n",
       "3                       3.7805                     3.5664   \n",
       "4                       6.1727                     5.8416   \n",
       "..                         ...                        ...   \n",
       "751                     3.0706                     3.0190   \n",
       "752                     1.9704                     1.7451   \n",
       "753                    51.5607                    44.4641   \n",
       "754                    19.1607                    12.8312   \n",
       "755                    62.9927                    21.8152   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                       3.8686                     4.2105   \n",
       "1                      23.5155                    14.1962   \n",
       "2                       9.4959                    10.7458   \n",
       "3                       5.2558                    14.0403   \n",
       "4                       6.0805                     5.7621   \n",
       "..                         ...                        ...   \n",
       "751                     3.1212                     2.4921   \n",
       "752                     1.8277                     2.4976   \n",
       "753                    26.1586                     6.3076   \n",
       "754                     8.9434                     2.2044   \n",
       "755                     9.2457                     4.8555   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                       5.1221                     4.4625   \n",
       "1                      11.0261                     9.5082   \n",
       "2                      11.0177                     4.8066   \n",
       "3                       4.2235                     4.6857   \n",
       "4                       7.7817                    11.6891   \n",
       "..                         ...                        ...   \n",
       "751                     3.5844                     3.5400   \n",
       "752                     5.2981                     4.2616   \n",
       "753                     2.8601                     2.5361   \n",
       "754                     1.9496                     1.9664   \n",
       "755                     3.0551                     3.0415   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                       2.6202                     3.0004   \n",
       "1                       6.5245                     6.3431   \n",
       "2                       2.9199                     3.1495   \n",
       "3                       4.8460                     6.2650   \n",
       "4                       8.2103                     5.0559   \n",
       "..                         ...                        ...   \n",
       "751                     3.3805                     3.2003   \n",
       "752                     6.3042                    10.9058   \n",
       "753                     3.5377                     3.3545   \n",
       "754                     2.6801                     2.8332   \n",
       "755                     4.0116                     2.6217   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_36  class  \n",
       "0                      18.9405      1  \n",
       "1                      45.1780      1  \n",
       "2                       4.7666      1  \n",
       "3                       4.0603      1  \n",
       "4                       6.1164      1  \n",
       "..                         ...    ...  \n",
       "751                     6.8671      0  \n",
       "752                    28.4170      0  \n",
       "753                     5.0424      0  \n",
       "754                     3.7131      0  \n",
       "755                     3.1527      0  \n",
       "\n",
       "[756 rows x 755 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('pd_speech_features.csv',header=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(columns = ['class','id']), data['class'], test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
    "x_test = pd.DataFrame(scaler.transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 8,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'presort': 'deprecated',\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf6 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=8)\n",
    "clf6.fit(x_train, y_train)\n",
    "clf6.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf6.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       123\n",
      "           1       0.97      1.00      0.98       406\n",
      "\n",
      "    accuracy                           0.97       529\n",
      "   macro avg       0.98      0.94      0.96       529\n",
      "weighted avg       0.97      0.97      0.97       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.59        69\n",
      "           1       0.81      0.91      0.86       158\n",
      "\n",
      "    accuracy                           0.79       227\n",
      "   macro avg       0.76      0.71      0.73       227\n",
      "weighted avg       0.78      0.79      0.78       227\n",
      "\n",
      "[[ 35  34]\n",
      " [ 14 144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_train, clf6.predict(x_train)))\n",
    "\n",
    "y_predict = clf6.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf7 = RandomForestClassifier(criterion='entropy')\n",
    "clf7.fit(x_train, y_train)\n",
    "clf7.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       123\n",
      "           1       1.00      1.00      1.00       406\n",
      "\n",
      "    accuracy                           1.00       529\n",
      "   macro avg       1.00      1.00      1.00       529\n",
      "weighted avg       1.00      1.00      1.00       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.49      0.63        69\n",
      "           1       0.81      0.97      0.88       158\n",
      "\n",
      "    accuracy                           0.82       227\n",
      "   macro avg       0.84      0.73      0.76       227\n",
      "weighted avg       0.83      0.82      0.81       227\n",
      "\n",
      "[[ 34  35]\n",
      " [  5 153]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, clf7.predict(x_train)))\n",
    "\n",
    "y_predict = clf7.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:19:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': -1,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.300000012,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 8,\n",
       " 'num_parallel_tree': 1,\n",
       " 'predictor': 'auto',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 1,\n",
       " 'tree_method': 'exact',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf8 = xgb.XGBClassifier()\n",
    "clf8.fit(x_train, y_train)\n",
    "clf8.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       123\n",
      "           1       1.00      1.00      1.00       406\n",
      "\n",
      "    accuracy                           1.00       529\n",
      "   macro avg       1.00      1.00      1.00       529\n",
      "weighted avg       1.00      1.00      1.00       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.59      0.73        69\n",
      "           1       0.85      0.99      0.91       158\n",
      "\n",
      "    accuracy                           0.87       227\n",
      "   macro avg       0.90      0.79      0.82       227\n",
      "weighted avg       0.88      0.87      0.86       227\n",
      "\n",
      "[[ 41  28]\n",
      " [  2 156]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, clf8.predict(x_train)))\n",
    "\n",
    "y_predict = clf8.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf9 = svm.SVC(C=5)\n",
    "clf9.fit(x_train, y_train)\n",
    "clf9.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       123\n",
      "           1       1.00      1.00      1.00       406\n",
      "\n",
      "    accuracy                           1.00       529\n",
      "   macro avg       1.00      1.00      1.00       529\n",
      "weighted avg       1.00      1.00      1.00       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.52      0.66        69\n",
      "           1       0.82      0.97      0.89       158\n",
      "\n",
      "    accuracy                           0.84       227\n",
      "   macro avg       0.86      0.75      0.78       227\n",
      "weighted avg       0.85      0.84      0.82       227\n",
      "\n",
      "[[ 36  33]\n",
      " [  4 154]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, clf9.predict(x_train)))\n",
    "\n",
    "y_predict = clf9.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       123\n",
      "           1       1.00      1.00      1.00       406\n",
      "\n",
      "    accuracy                           1.00       529\n",
      "   macro avg       1.00      1.00      1.00       529\n",
      "weighted avg       1.00      1.00      1.00       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70        69\n",
      "           1       0.87      0.85      0.86       158\n",
      "\n",
      "    accuracy                           0.81       227\n",
      "   macro avg       0.78      0.78      0.78       227\n",
      "weighted avg       0.81      0.81      0.81       227\n",
      "\n",
      "[[ 49  20]\n",
      " [ 23 135]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf1 = MLPClassifier(hidden_layer_sizes=(20,),activation='logistic',max_iter=1000)\n",
    "clf1.fit(x_train, y_train)\n",
    "print(metrics.classification_report(y_train, clf1.predict(x_train)))\n",
    "\n",
    "y_predict = clf1.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (20,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 1000,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELM(x,y,hidden_size=100,f=sigmoid):\n",
    "    \n",
    "    input_size = x.shape[1]\n",
    "    input_weights = np.random.normal(size=[input_size,hidden_size])\n",
    "    biases = np.random.normal(size=[hidden_size])\n",
    "    \n",
    "    output_weights = np.dot(pinv2(hidden_nodes(x,input_weights,biases,f)), y)\n",
    "    return input_weights,output_weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_nodes(x,w1,biases,actfunc=sigmoid):\n",
    "    G = np.dot(x, w1)\n",
    "    G = G + biases\n",
    "    H = actfunc(G)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w):\n",
    "    out = hidden_nodes(x,w[0],w[2])\n",
    "    out = np.dot(out, w[1])\n",
    "    return np.around(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = ELM(x_train,y_train,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.52       123\n",
      "           1       0.85      0.92      0.88       406\n",
      "\n",
      "    accuracy                           0.81       529\n",
      "   macro avg       0.74      0.68      0.70       529\n",
      "weighted avg       0.79      0.81      0.80       529\n",
      "\n",
      "[[ 55  68]\n",
      " [ 33 373]]\n"
     ]
    }
   ],
   "source": [
    "train_predict = predict(x_train,clf2)\n",
    "print(metrics.classification_report(y_train, train_predict))\n",
    "print(metrics.confusion_matrix(y_train, train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.45      0.54        69\n",
      "           1       0.79      0.91      0.85       158\n",
      "\n",
      "    accuracy                           0.77       227\n",
      "   macro avg       0.74      0.68      0.70       227\n",
      "weighted avg       0.76      0.77      0.75       227\n",
      "\n",
      "[[ 31  38]\n",
      " [ 14 144]]\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(x_test,clf2)\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = x_train.shape[1]\n",
    "visible = Input(shape=(n_inputs,))\n",
    "\n",
    "# encoder level 1\n",
    "e = Dense(n_inputs)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# encoder level 2\n",
    "e = Dense(n_inputs/4)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "\n",
    "# bottleneck\n",
    "n_bottleneck = round(float(n_inputs) / 8.0)\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "# define decoder, level 1\n",
    "d = Dense(n_inputs/4)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# decoder level 2\n",
    "d = Dense(n_inputs)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='sigmoid')(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.9450 - val_loss: 1.6631\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.8204 - val_loss: 1.6176\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7978 - val_loss: 1.5942\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7798 - val_loss: 1.5839\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7790 - val_loss: 1.5766\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7586 - val_loss: 1.5581\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7529 - val_loss: 1.5487\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7438 - val_loss: 1.5433\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.7453 - val_loss: 1.5417\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.7364 - val_loss: 1.5362\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7419 - val_loss: 1.5337\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7272 - val_loss: 1.5282\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7333 - val_loss: 1.5307\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7222 - val_loss: 1.5249\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7225 - val_loss: 1.5248\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7468 - val_loss: 1.5255\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.7210 - val_loss: 1.5201\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7217 - val_loss: 1.5193\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7126 - val_loss: 1.5219\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7105 - val_loss: 1.5261\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7212 - val_loss: 1.5205\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7095 - val_loss: 1.5176\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7097 - val_loss: 1.5173\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7077 - val_loss: 1.5161\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7042 - val_loss: 1.5184\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7144 - val_loss: 1.5177\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7193 - val_loss: 1.5185\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.7060 - val_loss: 1.5223\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7187 - val_loss: 1.5116\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7025 - val_loss: 1.5131\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7048 - val_loss: 1.5130\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.7064 - val_loss: 1.5150\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7064 - val_loss: 1.5126\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7029 - val_loss: 1.5124\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6985 - val_loss: 1.5170\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6994 - val_loss: 1.5110\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.7030 - val_loss: 1.5137\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6932 - val_loss: 1.5162\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6946 - val_loss: 1.5175\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.6898 - val_loss: 1.5163\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6957 - val_loss: 1.5136\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6975 - val_loss: 1.5145\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.7062 - val_loss: 1.5133\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6940 - val_loss: 1.5148\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.7018 - val_loss: 1.5077\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.6920 - val_loss: 1.5111\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6908 - val_loss: 1.5129\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.6996 - val_loss: 1.5128\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.7139 - val_loss: 1.5129\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6872 - val_loss: 1.5091\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.6897 - val_loss: 1.5099\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7026 - val_loss: 1.5105\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7072 - val_loss: 1.5141\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6858 - val_loss: 1.5114\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6858 - val_loss: 1.5145\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.6847 - val_loss: 1.5107\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6894 - val_loss: 1.5132\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6923 - val_loss: 1.5104\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6847 - val_loss: 1.5126\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6814 - val_loss: 1.5116\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6944 - val_loss: 1.5110\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6883 - val_loss: 1.5090\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.7051 - val_loss: 1.5103\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.6797 - val_loss: 1.5113\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.6960 - val_loss: 1.5119\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.7001 - val_loss: 1.5127\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.6805 - val_loss: 1.5109\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6909 - val_loss: 1.5096\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6933 - val_loss: 1.5103\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6797 - val_loss: 1.5128\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.6799 - val_loss: 1.5134\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6781 - val_loss: 1.5087\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6797 - val_loss: 1.5106\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6760 - val_loss: 1.5085\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6848 - val_loss: 1.5136\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6790 - val_loss: 1.5135\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6898 - val_loss: 1.5113\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6792 - val_loss: 1.5085\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6838 - val_loss: 1.5089\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6796 - val_loss: 1.5119\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6740 - val_loss: 1.5129\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6744 - val_loss: 1.5115\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6753 - val_loss: 1.5118\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6827 - val_loss: 1.5106\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6757 - val_loss: 1.5127\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6846 - val_loss: 1.5099\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6766 - val_loss: 1.5110\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6799 - val_loss: 1.5118\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6740 - val_loss: 1.5122\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6736 - val_loss: 1.5109\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6992 - val_loss: 1.5123\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6912 - val_loss: 1.5096\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.6727 - val_loss: 1.5086\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6758 - val_loss: 1.5104\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.6814 - val_loss: 1.5102\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6856 - val_loss: 1.5114\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6740 - val_loss: 1.5116\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6790 - val_loss: 1.5109\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.6873 - val_loss: 1.5118\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6730 - val_loss: 1.5119\n"
     ]
    }
   ],
   "source": [
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(loss='mse')\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(x_train, x_train, epochs=100,validation_data=(x_test,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 753)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 753)               567762    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 753)               3012      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 753)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 188)               141752    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 188)               752       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 188)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 94)                17766     \n",
      "=================================================================\n",
      "Total params: 731,044\n",
      "Trainable params: 729,162\n",
      "Non-trainable params: 1,882\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 94)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "x_train_encode = encoder.predict(x_train)\n",
    "x_test_encode = encoder.predict(x_test)\n",
    "\n",
    "clf3 = Perceptron()\n",
    "clf3.fit(x_train_encode, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72       123\n",
      "           1       0.94      0.86      0.90       406\n",
      "\n",
      "    accuracy                           0.85       529\n",
      "   macro avg       0.79      0.84      0.81       529\n",
      "weighted avg       0.87      0.85      0.86       529\n",
      "\n",
      "[[101  22]\n",
      " [ 56 350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69        69\n",
      "           1       0.86      0.87      0.87       158\n",
      "\n",
      "    accuracy                           0.81       227\n",
      "   macro avg       0.78      0.78      0.78       227\n",
      "weighted avg       0.81      0.81      0.81       227\n",
      "\n",
      "[[ 47  22]\n",
      " [ 20 138]]\n"
     ]
    }
   ],
   "source": [
    "train_predict = clf3.predict(x_train_encode)\n",
    "print(metrics.classification_report(y_train, train_predict))\n",
    "print(metrics.confusion_matrix(y_train, train_predict))\n",
    "\n",
    "prediction = clf3.predict(x_test_encode)\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       123\n",
      "           1       1.00      1.00      1.00       406\n",
      "\n",
      "    accuracy                           1.00       529\n",
      "   macro avg       1.00      1.00      1.00       529\n",
      "weighted avg       1.00      1.00      1.00       529\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.62      0.69        69\n",
      "           1       0.85      0.92      0.88       158\n",
      "\n",
      "    accuracy                           0.83       227\n",
      "   macro avg       0.81      0.77      0.78       227\n",
      "weighted avg       0.82      0.83      0.82       227\n",
      "\n",
      "[[ 43  26]\n",
      " [ 13 145]]\n"
     ]
    }
   ],
   "source": [
    "clf4 = MLPClassifier(hidden_layer_sizes=(20,),activation='logistic',max_iter=1000)\n",
    "clf4.fit(x_train_encode, y_train)\n",
    "print(metrics.classification_report(y_train, clf4.predict(x_train_encode)))\n",
    "\n",
    "y_predict = clf4.predict(x_test_encode)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.48      0.59       123\n",
      "           1       0.86      0.96      0.90       406\n",
      "\n",
      "    accuracy                           0.84       529\n",
      "   macro avg       0.81      0.72      0.75       529\n",
      "weighted avg       0.84      0.84      0.83       529\n",
      "\n",
      "[[ 59  64]\n",
      " [ 18 388]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.41      0.48        69\n",
      "           1       0.77      0.88      0.82       158\n",
      "\n",
      "    accuracy                           0.74       227\n",
      "   macro avg       0.68      0.64      0.65       227\n",
      "weighted avg       0.72      0.74      0.72       227\n",
      "\n",
      "[[ 28  41]\n",
      " [ 19 139]]\n"
     ]
    }
   ],
   "source": [
    "clf5 = ELM(x_train_encode,y_train,35)\n",
    "\n",
    "train_predict = predict(x_train_encode,clf5)\n",
    "print(metrics.classification_report(y_train, train_predict))\n",
    "print(metrics.confusion_matrix(y_train, train_predict))\n",
    "\n",
    "prediction = predict(x_test_encode,clf5)\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Fold Cross Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "scoring = ['accuracy','recall_weighted','precision_weighted','f1_weighted']\n",
    "\n",
    "x = data.drop(columns = ['class','id'])\n",
    "y = data['class']\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x))\n",
    "x_encode = encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "s2 = {'test_accuracy':[],'test_recall_weighted':[],'test_precision_weighted':[],'test_f1_weighted':[]}\n",
    "s5 = {'test_accuracy':[],'test_recall_weighted':[],'test_precision_weighted':[],'test_f1_weighted':[]}\n",
    "\n",
    "for i in range(5):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "    x_train_encode, x_test_encode, y_train_e, y_test_e = train_test_split(x_encode, y, test_size=0.20)\n",
    "    \n",
    "    clf2 = ELM(x_train,y_train,35)\n",
    "    clf5 = ELM(x_train_encode,y_train_e,35)\n",
    "    \n",
    "    y_pred2 = predict(x_test,clf2)\n",
    "    y_pred5 = predict(x_test_encode,clf5)\n",
    "    \n",
    "    s2['test_accuracy'].append(accuracy_score(y_test, y_pred2))\n",
    "    s5['test_accuracy'].append(accuracy_score(y_test_e, y_pred5))\n",
    "    \n",
    "    s2['test_recall_weighted'].append(recall_score(y_test, y_pred2, average='weighted'))\n",
    "    s5['test_recall_weighted'].append(recall_score(y_test_e, y_pred5, average='weighted'))\n",
    "\n",
    "    s2['test_precision_weighted'].append(precision_score(y_test, y_pred2, average='weighted'))\n",
    "    s5['test_precision_weighted'].append(precision_score(y_test_e, y_pred5, average='weighted'))\n",
    "    \n",
    "    s2['test_f1_weighted'].append(f1_score(y_test, y_pred2, average='weighted'))\n",
    "    s5['test_f1_weighted'].append(f1_score(y_test_e, y_pred5, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:37:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "clf1 = MLPClassifier(hidden_layer_sizes=(20,),activation='logistic',max_iter=1000)\n",
    "clf3 = Perceptron()\n",
    "clf4 = MLPClassifier(hidden_layer_sizes=(20,),activation='logistic',max_iter=1000)\n",
    "clf6 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=8)\n",
    "clf7 = RandomForestClassifier(criterion='entropy')\n",
    "clf8 = xgb.XGBClassifier()\n",
    "clf9 = svm.SVC(C=5)\n",
    "\n",
    "scores = []\n",
    "scores.append(cross_validate(clf1, x , y, scoring=scoring, cv=5))\n",
    "scores.append(s2)\n",
    "scores.append(cross_validate(clf3, x_encode , y, scoring=scoring, cv=5))\n",
    "scores.append(cross_validate(clf4, x_encode , y, scoring=scoring, cv=5))\n",
    "scores.append(s5)\n",
    "scores.append(cross_validate(clf6, x , y, scoring=scoring, cv=5))\n",
    "scores.append(cross_validate(clf7, x , y, scoring=scoring, cv=5))\n",
    "scores.append(cross_validate(clf8, x , y, scoring=scoring, cv=5))\n",
    "scores.append(cross_validate(clf9, x , y, scoring=scoring, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy\n",
      "0.7540606483095156\n",
      "0.781578947368421\n",
      "0.7699372603694666\n",
      "0.798963053328686\n",
      "0.7973684210526315\n",
      "0.7765249215754618\n",
      "0.8479086789822239\n",
      "0.8346897873823631\n",
      "0.8240937608922969\n",
      "\n",
      "test_precision_weighted\n",
      "0.7791295011812378\n",
      "0.7677275688908338\n",
      "0.7761171116321852\n",
      "0.7979132099839698\n",
      "0.7846570228928094\n",
      "0.7713207132269432\n",
      "0.8449532279970411\n",
      "0.8301780226227973\n",
      "0.8162207882871284\n",
      "\n",
      "test_recall_weighted\n",
      "0.7540606483095156\n",
      "0.781578947368421\n",
      "0.7699372603694666\n",
      "0.798963053328686\n",
      "0.7973684210526315\n",
      "0.7765249215754618\n",
      "0.8479086789822239\n",
      "0.8346897873823631\n",
      "0.8240937608922969\n",
      "\n",
      "test_f1_weighted\n",
      "0.7614582844083835\n",
      "0.766387372925174\n",
      "0.7693356082893057\n",
      "0.7955953222997698\n",
      "0.7861275064243317\n",
      "0.7735328367035613\n",
      "0.8353708258993015\n",
      "0.8240332117824863\n",
      "0.8147115144148211\n"
     ]
    }
   ],
   "source": [
    "print('test_accuracy')\n",
    "for s in scores:\n",
    "    print(np.mean(s['test_accuracy']))\n",
    "print()\n",
    "print('test_precision_weighted')\n",
    "for s in scores:\n",
    "    print(np.mean(s['test_precision_weighted']))\n",
    "print()\n",
    "print('test_recall_weighted')\n",
    "for s in scores:\n",
    "    print(np.mean(s['test_recall_weighted']))  \n",
    "print()\n",
    "print('test_f1_weighted')\n",
    "for s in scores:\n",
    "    print(np.mean(s['test_f1_weighted']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:23:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[102  90]\n",
      " [ 35 529]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = cross_val_predict(clf8, x, y, cv=5)\n",
    "print(metrics.confusion_matrix(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
